apiVersion: serving.kserve.io/v1alpha1
kind: LLMInferenceService
metadata:
  name: model-b
  namespace: llm
  labels:
    tier: premium
spec:
  model:
    name: "model-b"
    uri: "pvc://empty-model"
  template:
    containers:
    - name: kserve-container
      image: ghcr.io/llm-d/llm-d-inference-sim:v0.5.1
      ports:
      - containerPort: 8000
        protocol: TCP
      command: ["/app/llm-d-inference-sim"]
      args:
      - "--port"
      - "8000"
      - "--model"
      - "model-b"
      - "--mode"
      - "random"
      resources:
        requests:
          cpu: "100m"
          memory: "128Mi"
        limits:
          cpu: "500m"
          memory: "256Mi"
