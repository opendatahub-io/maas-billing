# Sample LLMInferenceService Models

This directory contains sample model deployments using KServe's `LLMInferenceService` custom resource for the MaaS platform.

## Prerequisites

- **OpenDataHub (ODH) or Red Hat OpenShift AI (RHOAI)** with KServe enabled
- **Namespace**: `llm` namespace must exist
- **Infrastructure**: Core MaaS infrastructure deployed (Gateway, MaaS API, policies)

## Available Models

### 1. Simulator (CPU-based)
**Path**: `simulator/`
- Uses `ghcr.io/llm-d/llm-d-inference-sim:v0.5.1` image
- Minimal resources (500m CPU, 1Gi memory)
- Perfect for testing infrastructure without real model overhead
- **Status**: ✅ Working

### 2. Facebook OPT-125M (CPU-based)
**Path**: `facebook-opt-125m-cpu/`
- Uses `quay.io/pierdipi/vllm-cpu:latest` image
- Small language model for CPU inference
- Requires 1 CPU, 10Gi memory
- **Status**: ⚠️ Running but readiness check issues

### 3. Qwen3-0.6B (GPU-based)
**Path**: `qwen3/`
- Requires NVIDIA GPU nodes
- 0.6B parameter model
- Requires 1 GPU, 8-16Gi memory
- **Status**: ⚠️ Requires GPU nodes

## Directory Structure

```
models/
├── rbac/                    # Shared RBAC configuration
│   ├── all-tiers.yaml      # Role and RoleBinding for tier access
│   └── kustomization.yaml
├── simulator/
│   ├── model.yaml          # LLMInferenceService definition
│   └── kustomization.yaml
├── facebook-opt-125m-cpu/
│   ├── model.yaml
│   └── kustomization.yaml
└── qwen3/
    ├── model.yaml
    └── kustomization.yaml
```

## Deployment

### Deploy All Models
```bash
# Deploy simulator (recommended for testing)
kubectl apply -k deployment/samples/models/simulator/

# Deploy Facebook OPT-125M (CPU)
kubectl apply -k deployment/samples/models/facebook-opt-125m-cpu/

# Deploy Qwen3 (requires GPU)
kubectl apply -k deployment/samples/models/qwen3/
```

## OpenShift Routes

Each model now includes an OpenShift Route in its kustomization for direct access. The routes are automatically created when deploying with `kubectl apply -k`.

**Important Note**: The service names generated by KServe don't always follow the namePrefix pattern from kustomization. The route files are configured to use the actual service names created by KServe.

### Route Access
After deployment, you can access the models via their OpenShift routes:
```bash
# List all routes
kubectl get routes -n llm

# Example URLs (actual hostnames depend on your cluster domain):
# - https://qwen3-single-node-no-scheduler-nvidia-gpu-route-llm.apps.your-cluster.com
# - https://facebook-opt-125m-simulated-route-llm.apps.your-cluster.com
# - https://facebook-opt-125m-cpu-route-llm.apps.your-cluster.com
```

### Troubleshooting Routes
If routes are not working:
1. Check that the service names in the route match what KServe created:
   ```bash
   kubectl get svc -n llm
   ```
2. Verify the route points to the correct service:
   ```bash
   kubectl describe route <route-name> -n llm
   ```

### Verify Deployment
```bash
# Check LLMInferenceService status
kubectl get llminferenceservices -n llm

# Expected output for working models:
# NAME                          URL                                    READY
# facebook-opt-125m-simulated   http://...elb.amazonaws.com/llm/...   True

# Check pods
kubectl get pods -n llm

# View logs
kubectl logs -n llm <pod-name>
```

## Architecture

Each model uses the same pattern:

1. **LLMInferenceService** (`model.yaml`): Defines the model serving configuration
2. **Kustomization** (`kustomization.yaml`): 
   - Sets namespace to `llm`
   - Applies namePrefix for resource naming
   - Includes shared RBAC from `../rbac/`
   - Patches RBAC for model-specific permissions

### Shared RBAC

The `rbac/` directory contains shared Role and RoleBinding definitions that grant tier-based access:
- `system:serviceaccounts:openshift-ai-inference-tier-free`
- `system:serviceaccounts:openshift-ai-inference-tier-premium`
- `system:serviceaccounts:openshift-ai-inference-tier-enterprise`

Each model's kustomization patches these to add model-specific resource names.

## Troubleshooting

### Common Issues

1. **IstioDestinationRuleReconcileError**
   - Ensure Istio/Service Mesh is running: `kubectl get pods -n istio-system`
   - Restart Istio if needed: `kubectl rollout restart deployment istiod -n istio-system`

2. **Pods not starting**
   - Check if ODH/KServe controllers are running: `kubectl get pods -n opendatahub | grep kserve`
   - Check events: `kubectl describe llminferenceservice <name> -n llm`

3. **Readiness probe failures**
   - Some vLLM images may not implement `/ready` endpoint
   - Check logs: `kubectl logs -n llm <pod-name>`

4. **GPU models not scheduling**
   - Verify GPU nodes: `kubectl get nodes -L nvidia.com/gpu.present`
   - Check GPU operator: `kubectl get pods -n gpu-operator-resources`

## Testing Models

### Test Simulator
```bash
# Port-forward to test locally
kubectl port-forward -n llm svc/facebook-opt-125m-simulated-kserve-workload-svc 8000:443

# In another terminal, test the endpoint
curl -k https://localhost:8000/health
```

### Access Through Gateway
Models are accessible through the MaaS API gateway with proper authentication:
```bash
# Get token from MaaS API first
TOKEN=$(curl -sSk -H "Authorization: Bearer $(oc whoami -t)" \
  -X POST -d '{"expiration": "10m"}' \
  "https://maas-api.apps.your-cluster.com/v1/tokens" | jq -r .token)

# Access model through gateway
curl -k -H "Authorization: Bearer $TOKEN" \
  "https://gateway.apps.your-cluster.com/llm/facebook-opt-125m-simulated/v1/chat/completions" \
  -d '{"model": "simulated", "messages": [{"role": "user", "content": "Hello"}]}'
```

## Notes

- These models were migrated from `maas-api/deploy/models/` which contains the proven working configurations
- The simulator model is the most reliable for testing as it doesn't require downloading large model files
- GPU models require appropriate node resources and GPU operator installation 