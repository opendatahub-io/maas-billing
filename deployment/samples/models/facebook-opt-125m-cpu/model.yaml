apiVersion: serving.kserve.io/v1alpha1
kind: LLMInferenceService
metadata:
  name: single-node-no-scheduler-cpu
spec:
  model:
    uri: hf://facebook/opt-125m
    name: facebook/opt-125m
  replicas: 1
  router:
    route: { }
  template:
    containers:
      - name: main
        image: quay.io/pierdipi/vllm-cpu:latest
        env:
          - name: VLLM_LOGGING_LEVEL
            value: DEBUG
          # OPT models don't have a built-in chat template
          # Provide a simple chat template for OPT models
          - name: VLLM_CHAT_TEMPLATE
            value: |
              {% for message in messages %}
              {% if message['role'] == 'user' %}User: {{ message['content'] }}
              {% elif message['role'] == 'assistant' %}Assistant: {{ message['content'] }}
              {% elif message['role'] == 'system' %}System: {{ message['content'] }}
              {% endif %}
              {% endfor %}
              Assistant:
        resources:
          limits:
            cpu: '1'
            memory: 10Gi
          requests:
            cpu: '100m'
            memory: 8Gi
        livenessProbe:
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 30
          failureThreshold: 5
